#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import csv
import zipfile
import io
import numpy as np
import faiss
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler
from lightgbm import LGBMClassifier
from pathlib import Path

# ================== 1.è·¯å¾„é…ç½® (Pathlib ç‰ˆ) ==================
# 1. è‡ªåŠ¨å®šä½é¡¹ç›®æ ¹ç›®å½• (.../SRTP/)
#    è§£é‡Š: å½“å‰è„šæœ¬ -> matching -> src -> SRTP
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

# 2. å®šä¹‰å„çº§æ•°æ®ç›®å½•
DATA_DIR = PROJECT_ROOT / "data" / "raw"
PROCESSED_DIR = PROJECT_ROOT / "data" / "processed"
RESULTS_DIR = PROJECT_ROOT / "data" / "results"

# 3. å…·ä½“æ–‡ä»¶è·¯å¾„
ARCHIVE_PATH = DATA_DIR / "kt3.tar.gz"
QUESTION_FILE = DATA_DIR / "questions.csv"

DIFF_FILE = PROCESSED_DIR / "question_difficulty.csv"
OUT_FILE = RESULTS_DIR / "matched_difficulty.csv"

# (å¯é€‰) è‡ªåŠ¨åˆ›å»ºç»“æœç›®å½•ï¼Œé˜²æ­¢æŠ¥é”™
RESULTS_DIR.mkdir(parents=True, exist_ok=True)
# ==========================================================
# ==========================================
# 2. æ ¸å¿ƒå‚æ•°é…ç½®
# ==========================================
LIMIT_FILES = 1000        # è¯»å–å¤šå°‘ä¸ªç”¨æˆ·æ–‡ä»¶ (è°ƒè¯•æ—¶è®¾å°ï¼Œè·‘å…¨é‡è®¾ä¸º None)
HINT_WINDOW_MS = 10 * 60 * 1000  # 10åˆ†é’Ÿå†…çš„å­¦ä¹ è¡Œä¸ºç®—ä½œçœ‹æç¤º
SEARCH_CANDIDATES = 50    # FAISS åˆæœå¯»æ‰¾ 50 ä¸ª PS æœ€è¿‘é‚»
PS_CALIPER = 0.05         # PS åˆ†æ•°å·®å¼‚å®¹å¿åº¦ (èƒ½åŠ›å·®å¼‚)
DIFF_CALIPER = 0.1        # é¢˜ç›®éš¾åº¦å·®å¼‚å®¹å¿åº¦ (ä»»åŠ¡å·®å¼‚) <--- æ ¸å¿ƒæ”¹è¿›
# ==========================================

def load_resources():
    """åŠ è½½é¢˜ç›®ç­”æ¡ˆå’Œéš¾åº¦è¡¨"""
    print(f"ğŸ“– [1/5] åŠ è½½èµ„æºæ–‡ä»¶...")
    
    # 1. åŠ è½½é¢˜ç›®ç­”æ¡ˆ
    q_map = {}
    if os.path.exists(QUESTION_FILE):
        with open(QUESTION_FILE, 'r', encoding='utf-8') as f:
            for row in csv.DictReader(f):
                q_map[row['question_id']] = row['correct_answer']
        print(f"   -> å·²åŠ è½½ {len(q_map)} é“é¢˜ç›®ç­”æ¡ˆ")
    else:
        print(f"   -> é”™è¯¯: æ‰¾ä¸åˆ° {QUESTION_FILE}")
        return None, None

    # 2. åŠ è½½é¢˜ç›®éš¾åº¦
    d_map = {}
    if os.path.exists(DIFF_FILE):
        with open(DIFF_FILE, 'r') as f:
            for row in csv.DictReader(f):
                d_map[row['item_id']] = float(row['avg_correctness'])
        print(f"   -> å·²åŠ è½½ {len(d_map)} æ¡éš¾åº¦æ•°æ®")
    else:
        print(f"   -> è­¦å‘Š: æ‰¾ä¸åˆ°éš¾åº¦è¡¨ {DIFF_FILE}ï¼Œå°†æ— æ³•è¿›è¡Œéš¾åº¦å¯¹é½ï¼")
        # ä¸ºäº†ä»£ç ä¸å´©ï¼Œç»™ä¸ªç©ºå­—å…¸ï¼Œåç»­ä¼šé»˜è®¤ difficulty=0.5
    
    return q_map, d_map

def extract_features(q_map, d_map):
    """ä»å‹ç¼©åŒ…æµå¼æå–ç‰¹å¾"""
    print(f"[2/5] æ­£åœ¨ä» {os.path.basename(ARCHIVE_PATH)} æå–ç‰¹å¾...")
    
    data_rows = []
    
    if not os.path.exists(ARCHIVE_PATH):
        print(f"æ‰¾ä¸åˆ°æ•°æ®åŒ…: {ARCHIVE_PATH}")
        return []

    with zipfile.ZipFile(ARCHIVE_PATH, 'r') as zf:
        files = [f for f in zf.namelist() if f.endswith('.csv') and not f.endswith('/')]
        if LIMIT_FILES: 
            files = files[:LIMIT_FILES]
            print(f"   -> é‡‡æ ·æ¨¡å¼: ä»…å¤„ç†å‰ {LIMIT_FILES} ä¸ªç”¨æˆ·")
        
        for member_name in tqdm(files, desc="Processing Users"):
            with zf.open(member_name) as f:
                with io.TextIOWrapper(f, encoding='utf-8', errors='replace') as tf:
                    reader = csv.DictReader(tf)
                    
                    # ç”¨æˆ·çŠ¶æ€å˜é‡
                    history_correct = 0
                    history_total = 0
                    last_action_ts = 0
                    learning_timestamps = [] # è®°å½•çœ‹ e/l çš„æ—¶é—´
                    
                    for row in reader:
                        try:
                            ts = int(row['timestamp'])
                            action = row['action_type']
                            item = row['item_id']
                            
                            # è®°å½•å­¦ä¹ è¡Œä¸º (Explanation / Lecture)
                            if action == 'enter' and (item.startswith('e') or item.startswith('l')):
                                learning_timestamps.append(ts)
                                last_action_ts = ts
                                continue
                            
                            # å¤„ç†ç­”é¢˜è¡Œä¸º
                            if action == 'respond' and item.startswith('q'):
                                # 1. è®¡ç®—å“åº”æ—¶é—´
                                elapsed = ts - last_action_ts if last_action_ts > 0 else 0
                                elapsed = max(0, min(elapsed, 300000)) # æˆªæ–­å¼‚å¸¸å€¼
                                
                                # 2. åˆ¤æ–­æ˜¯å¦çœ‹è¿‡æç¤º (Recall Window)
                                hint_used = 0
                                # å€’åºæ£€æŸ¥æœ€è¿‘çš„å­¦ä¹ è®°å½•
                                for l_ts in reversed(learning_timestamps):
                                    if ts - l_ts > HINT_WINDOW_MS: break # è¶…æ—¶äº†
                                    if ts - l_ts >= 0:
                                        hint_used = 1
                                        break
                                
                                # 3. è®¡ç®— Outcome
                                correct_ans = q_map.get(item)
                                if not correct_ans: continue # æ²¡ç­”æ¡ˆçš„é¢˜è·³è¿‡
                                is_correct = 1 if row.get('user_answer') == correct_ans else 0
                                
                                # 4. å†å²æ­£ç¡®ç‡ (Ability Proxy)
                                acc_rate = history_correct / history_total if history_total > 0 else 0.0
                                
                                # 5. é¢˜ç›®éš¾åº¦
                                diff_val = d_map.get(item, 0.5) # é»˜è®¤ä¸­ç­‰éš¾åº¦

                                # æ”¶é›†ä¸€è¡Œæ•°æ®
                                data_rows.append([
                                    acc_rate,                   # Feature 0: å†å²æ­£ç¡®ç‡
                                    np.log1p(history_total),    # Feature 1: åšé¢˜ç»éªŒ(log)
                                    np.log1p(elapsed),          # Feature 2: è€—æ—¶(log)
                                    diff_val,                   # Feature 3: é¢˜ç›®éš¾åº¦ (ä½œä¸ºç‰¹å¾ä¹Ÿè¦æ”¾è¿›æ¨¡å‹)
                                    hint_used,                  # Treatment
                                    is_correct                  # Outcome
                                ])
                                
                                # æ›´æ–°çŠ¶æ€
                                history_total += 1
                                history_correct += is_correct
                                last_action_ts = ts
                                
                                # é™åˆ¶å†…å­˜ï¼Œåªä¿ç•™æœ€è¿‘çš„å­¦ä¹ è®°å½•
                                if len(learning_timestamps) > 20:
                                    learning_timestamps = learning_timestamps[-10:]
                            else:
                                last_action_ts = ts
                                
                        except ValueError: continue
                        
    return np.array(data_rows)

def main():
    # 1. åŠ è½½èµ„æº
    q_map, d_map = load_resources()
    if not q_map: return

    # 2. æå–ç‰¹å¾
    data = extract_features(q_map, d_map)
    if len(data) == 0:
        print("æ²¡æœ‰æå–åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥åŸå§‹æ•°æ®åŒ…ã€‚")
        return

    # æ•°æ®åˆ‡åˆ†
    # X: [acc, log_count, log_time, difficulty]
    X = data[:, :4] 
    W = data[:, 4]   # Treatment (Hint)
    Y = data[:, 5]   # Outcome
    DIFF_COL = data[:, 3] # å•ç‹¬æ‹¿å‡ºæ¥æ–¹ä¾¿åé¢ç­›é€‰
    
    print(f"[3/5] æ•°æ®ç»Ÿè®¡: æ€»æ ·æœ¬ {len(data)}")
    print(f"   - Treated (Hint): {int(sum(W))}")
    print(f"   - Control (No Hint): {len(W) - int(sum(W))}")

    # 3. è®¡ç®— Propensity Score
    print("[4/5] è®­ç»ƒ LightGBM è®¡ç®—å€¾å‘æ€§å¾—åˆ† (PS Score)...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    clf = LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)
    clf.fit(X_scaled, W)
    ps_scores = clf.predict_proba(X_scaled)[:, 1]

    # 4. æ‰§è¡Œ 1:1 åŒé‡å¡å°ºåŒ¹é…
    print(f"[5/5] æ‰§è¡Œ 1:1 ç²¾ç¡®åŒ¹é… (PS < {PS_CALIPER} & Diff < {DIFF_CALIPER})...")
    
    treated_indices = np.where(W == 1)[0]
    control_indices = np.where(W == 0)[0]
    
    # å»ºç«‹ç´¢å¼• (ç”¨ Treated åšåº“)
    treated_ps_vec = ps_scores[treated_indices].reshape(-1, 1).astype('float32')
    index = faiss.IndexFlatL2(1) # L2 è·ç¦»å…¶å®å°±æ˜¯ 1D çš„ç»å¯¹å·®çš„å¹³æ–¹
    index.add(treated_ps_vec)
    
    control_ps_vec = ps_scores[control_indices].reshape(-1, 1).astype('float32')
    
    # æœç´¢ K ä¸ªæœ€è¿‘é‚» (K è¦å¤§ä¸€ç‚¹ï¼Œå› ä¸ºæˆ‘ä»¬è¦ä»ä¸­ç­›é€‰ç¬¦åˆéš¾åº¦è¦æ±‚çš„)
    D, I = index.search(control_ps_vec, SEARCH_CANDIDATES)
    
    matched_pairs = []
    used_treated_indices = set() # ä¿è¯ 1:1ï¼Œä¸é‡å¤ä½¿ç”¨ Treated æ ·æœ¬
    
    # éå†æ¯ä¸€ä¸ª Control æ ·æœ¬å¯»æ‰¾å®ƒçš„ Soul Mate
    for i in range(len(control_indices)):
        c_real_idx = control_indices[i]
        c_diff = DIFF_COL[c_real_idx] # Control åšçš„é¢˜çš„éš¾åº¦
        
        # éå†å®ƒçš„ K ä¸ª PS é‚»å±…
        best_match = None
        
        for k in range(SEARCH_CANDIDATES):
            dist_sq = D[i][k]
            t_idx_subset = I[i][k]
            
            if t_idx_subset == -1: continue
            
            # 1. æ£€æŸ¥ PS å¡å°º
            if dist_sq > PS_CALIPER**2: 
                continue # PS å·®å¤ªè¿œ
            
            t_real_idx = treated_indices[t_idx_subset]
            
            # 2. æ£€æŸ¥æ˜¯å¦å·²ç»è¢«ç”¨è¿‡äº† (1:1 çº¦æŸ)
            if t_real_idx in used_treated_indices:
                continue
            
            # 3. æ£€æŸ¥éš¾åº¦å¡å°º (æ ¸å¿ƒé€»è¾‘!)
            t_diff = DIFF_COL[t_real_idx]
            if abs(t_diff - c_diff) > DIFF_CALIPER:
                continue # é¢˜ç›®éš¾åº¦å·®å¤ªè¿œ (è‹¹æœ vs æ©˜å­)ï¼Œè·³è¿‡
            
            # å¦‚æœéƒ½æ»¡è¶³ï¼Œè¿™æ‰æ˜¯æˆ‘ä»¬è¦çš„äºº
            best_match = t_real_idx
            used_treated_indices.add(t_real_idx) # æ ‡è®°ä¸ºå·²ç”¨
            break # æ‰¾åˆ°ä¸€ä¸ªå°±å¤Ÿäº†ï¼Œé€€å‡ºå†…å±‚å¾ªç¯
        
        if best_match is not None:
            # è®°å½•åŒ¹é…å¯¹
            t_real_idx = best_match
            matched_pairs.append({
                't_idx': t_real_idx,
                'c_idx': c_real_idx,
                'ps_t': ps_scores[t_real_idx],
                'ps_c': ps_scores[c_real_idx],
                'diff_t': DIFF_COL[t_real_idx],
                'diff_c': DIFF_COL[c_real_idx],
                'outcome_t': Y[t_real_idx],
                'outcome_c': Y[c_real_idx]
            })

    # 5. ä¿å­˜ç»“æœ
    print(f"ä¿å­˜ç»“æœè‡³ {OUT_FILE}...")
    print(f"   -> åŸå§‹ Control æ•°: {len(control_indices)}")
    print(f"   -> åŒ¹é…æˆåŠŸå¯¹æ•°: {len(matched_pairs)} (ä¸¢å¼ƒç‡: {100 - len(matched_pairs)/len(control_indices)*100:.1f}%)")
    
    if matched_pairs:
        keys = matched_pairs[0].keys()
        with open(OUT_FILE, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            writer.writerows(matched_pairs)
        print("å…¨éƒ¨å®Œæˆï¼å»ç”»å›¾å§ï¼")
    else:
        print("åŒ¹é…å¤±è´¥ï¼Œæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç¬¦åˆæ¡ä»¶çš„æ ·æœ¬å¯¹ã€‚è¯•ç€æ”¾å®½ CALIPERï¼Ÿ")

if __name__ == '__main__':
    main()